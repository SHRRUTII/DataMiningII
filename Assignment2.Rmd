---
title: "Kmeans_part1"
author: "Shruti Sharma"
date: "5/17/2020"
output:
  html_document:
    df_print: paged
---
importing packages and fetching data
```{r setup, include=FALSE}
options(repos=structure(c(CRAN="YOUR FAVORITE MIRROR")))
install.packages("knitr",repos = "http://cran.us.r-project.org")
knitr::opts_chunk$set(echo = TRUE)
install.packages('tidyverse')
install.packages('ggplot2')
library(tidyverse)
library(ggplot2)
options(digits = 2)
expand.grid(c(-2,2),c(-2,2),c(-2,2))
setwd("C:/Users/shrut/Documents/DataMiningII-6704/")
getwd()
install.packages('here')
library(here)
insurancedata=read_csv(here::here("Assignment2/insurance.csv"))
view(insurancedata)
```
As the data contains categorical data too, first step was to encode the data using fastDummies package.
```{r}
is.na(insurancedata)
sapply(insurancedata, class)#checking data type 
library(fastDummies)
encoded_data <- fastDummies::dummy_cols(insurancedata)
knitr::kable(encoded_data)
view(encoded_data)
temp_variable=encoded_data
```
Then the data has some outliers and as kmeans is sensitive to outliers, second step was to minimize outliers, which was achieved by using the dplyr package
```{r }
#Removing outliers
library(dplyr)
remove_outliers <- function(temp_variable, na.rm = TRUE) {
  qnt <- quantile(temp_variable, probs=c(.25, .75), na.rm = na.rm)
  H <- 1.5 * IQR(temp_variable, na.rm = na.rm)
  new <- temp_variable
  new[temp_variable < (qnt[1] - H)] <-0
  new[temp_variable > (qnt[2] + H)] <- 0
  new
}
temp_variable$charges=remove_outliers(temp_variable$charges)
view(temp_variable)
#boxplot(temp_variable)
outliers<-boxplot(temp_variable$charges, plot=FALSE)$out
temp_variable<- temp_variable[-which(temp_variable$charges %in% outliers),]
temp_variable$charges[temp_variable$charges==0] <-median(temp_variable$charges, na.rm=TRUE)
temp_variable <- temp_variable[order(temp_variable$charges),]
```

According to the general data understanding, we will keep the parameters most affecting insurance charges in the dataset we will be working on:
```{r}
v_keep <- c("age","bmi", "children" , "charges")
I <- temp_variable[,v_keep]
view(I)
```
Now using kmeans() function, without scaling and custom initialisation of data, we will try to find the clusters
```{r}
kmeans_insurance_1 <- kmeans(
  x = I,
  centers = 2
)
kmeans_insurance_1
```
Since, scaling is not done, we can cleraly see that center for charges column is dominating rest 3 centers(age ,bmi and children). To avoid working of algorithm focusing on charges and having equal influence on data we would normalise our data by scaling and then running kmeans() again as:
```{r}
#Scaling:
scale_I <- scale(I)
kmeans_insurance_2 <- kmeans(
  x = scale_I,
  centers = 2
)
kmeans_insurance_2
```
From above, we can say that the data is normalised.Also, the wss is ok(27.4%) but not that good, so weed to work on it too.The issue persist of randomness of center,so to avoid this and to produce reproducible result and also to improve our wss, we would like to provide, cutsomise centers for our kmeans() as:
```{r}
#avoiding random centers
v <-c(3.8,7,-8.3,-4.99)
initial_centers <- matrix(
  data = c(-abs(v)/2,abs(v)/3,v/2,-v/3),
  nrow = 4,
  byrow = TRUE
)
kmeans_insurance_3 <- kmeans(
  x = scale_I,
  centers = initial_centers
)
kmeans_insurance_3
```
The result is now, reproducible, also the within cluster sum of squares of cluster is improved much from the normalised but not customised centers. Also, we can see that the clusters are quiet in a good condition as, the cluster 1 is at the beginning and in the end we have cluster 3,4, so it seems quiet promising.Now we will do PCA for our data as:
```{r}
#PCA
prcomp_I <- data.frame(
  prcomp(
    x = scale_I,
    center = FALSE,
    scale. = FALSE
  )$x[,1:2],
  
  Age= temp_variable$age,
  Charges= temp_variable$charges,
  Cluster = as.character(kmeans_insurance_3$cluster),
  stringsAsFactors = FALSE
)
require(ggplot2)
## Loading required package: ggplot2
require(ggforce)
## Loading required package: ggforce
ggplot(prcomp_I) + 
  aes(x = PC1,y = PC2,size = Charges,color = Cluster,fill = Cluster,label="",group = Cluster) + 
  geom_point(alpha=0.6) + 
  ggrepel::geom_text_repel(color = "black",size = 1) + 
  ggtitle("Scatter plot of Insurance principal components","Color corresponds to k-means cluster") + 
  theme_bw() + 
  theme(legend.position = "top")
```
The clusters are quite visible, cluster 3 and 4 does have some little overlapping,but considering wss and cluster division, we can consider this as quite good cluster, but still we need to confirm the better cluster size by running kmeans for different clusters and then finding the best among all.Lets see which cluster size give us better wss along with proper cluster size and cluster division. 
Lets start with cluster size=2
```{r}
#### 2 clusters
kmeans(
  x = scale_I,
  centers = 2
)
```
We got nearly same size clusters with wss of 27.5%(random centers) which is ok. Now, we would go with 3 cluster size:
```{r}
kmeans(
  x = scale_I,
  centers = 3
)
```
The clusters are of almost equal size and wssgot improved too. Also, the clustering shows quite good partition. Lets see for cluster size=4 with customised centers:
```{r}
#avoiding random centers
v <-c(3.8,7,-8.3,-4.99)
initial_centers <- matrix(
  data = c(-abs(v)/2,abs(v)/3,v/2,-v/3),
  nrow = 4,
  byrow = TRUE
)
colnames(initial_centers) <- v_keep
initial_centers
```
After initialising centers,lets see the kmeans output:

```{r}
kmeans_insurance_3 <- kmeans(
  x = scale_I,
  centers = initial_centers
)
kmeans_insurance_3
```
We got quite improved wss and good clutser division.
The next step invloves plotting PCA to see our clusters , as visualiztion is a best way to interprete the output:

```{r}
install.packages('useful')
library(useful)
useful::plot.kmeans(
  x = kmeans_insurance_3,
  data = scale_I 
)
```
We can see that we are getting quite a good clusters. We can finalise, cluster size=4 is better cluster size.We can see these clusters with different pacakages as cluster, fpc, factoextra. 

```{r}
cluster::clusplot(
  scale_I,
  kmeans_insurance_3$cluster,
  color=TRUE,
  shade=TRUE,
  lines=0
)
```
Now we will use fpc pacakage for clusterplot
```{r}
install.packages('fpc')
library(fpc)
fpc::plotcluster(
  x = scale_I,
  clvecd = kmeans_insurance_3$cluster
)
```
Due to huge data, we can drop fpc cluster package as it is not giving us a clear idea of the cluster. Next we will go with factoextra package
```{r}
install.packages('factoextra')
library(factoextra)
factoextra::fviz_cluster(
  object = kmeans_insurance_3,
  data = scale_I,
  repel=TRUE ## to avoid overplotting due to labels
  
)
```
with labels:
```{r}
install.packages('factoextra')
library(factoextra)
factoextra::fviz_cluster(
  object = kmeans_insurance_3,
  data = scale_I
)
```
We can also find better cluster size by using different methods like elbow, silhouette, gap and cluster statistics.
```{r}
set.seed(823)
factoextra::fviz_nbclust(
  x = scale_I,
  FUNcluster = kmeans,
  method = "wss"
)

```
From the graph, we can see clutser size=2 is shown to give more promising result. We will now go for Silhouette which computes on the basis of which data point fits into the cluster.

```{r}
set.seed(823)
factoextra::fviz_nbclust(
  x = scale_I,
  FUNcluster = kmeans,
  method = "silhouette"
)

```
From Silhouette, we can deduce that cluster size= 4to be proving more promising results. For intracluster variation we can go with Gap Statistics as:

```{r}
set.seed(823)
clusGap_kmeans <- cluster::clusGap(
  x = scale_I,
  FUNcluster = kmeans,
  K.max =50,iter.max=50
)
clusGap_kmeans

```
Plotting for cluster size
```{r}
factoextra::fviz_gap_stat(
  gap_stat = clusGap_kmeans
)

```
We can deduce from graph that cluster size= 11 is giving more promising results. With gap statistics method, we can see the cluster size as:

```{r}
set.seed(823)
factoextra::fviz_nbclust(
  x = scale_I,
  FUNcluster = kmeans,
  method = "gap_stat"
)

```
By choosing gap statistics method we can say that cluster size=1 is a better option.Lets move towards cluster statistics
.
```{r}
fpc::cluster.stats(
  d = dist(scale_I),
  clustering = kmeans_insurance_3$cluster,
  G2 = TRUE,
  G3 = TRUE
)

```
We can deduce that cluster size=4 is better option.
For kmeans, we can deduce that cluster size=4 is a better option as we got wss good for cluster size=4 also cluster statistics also indicates the same.So, we will consider cluster size=4 for kmeans()
####################################################Various Function#########################################################
We will start with scaling and ordering our data
```{r}
v <- prcomp(scale_I)$x[,1]
scale_I <- scale_I[order(v),]

```
As the data is of large size, we will try to cluster using clara function

```{r}
clara_I <- cluster::clara(
  x = scale_I,
  k = 4
)
plot(clara_I)

```
We can deduce from Silhouette plot that clusters are properly maintainedas almost all bars in the right side and only two in left side. That coukd be either due to outlier or misplacement of point in that cluster.To get the details, we can print the clara data as:

```{r}
print(clara_I)
```
To deduce likelihood of the points we can use fanny function as:

```{r}
fanny_I <- cluster::fanny(
  x = scale_I,
  k = 4,
  memb.exp =1.5,
  
)

```
Plotting the function gives clear idea about the clustering scenario:
```{r}
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/fanny.pdf',height=30,width=30)
plot(fanny_I)
dev.off()
```
printing the data
```{r}
plot(fanny_I)
print(fanny_I)
```
Fanny is not giving a better output as memebership coefficients are 25% each, so we can ignore Fanny. Other option is pam

```{r}
pam_I <- cluster::pam(I,k = 4)
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/pam.pdf')
plot(pam_I)
dev.off()
```
plots are exported in pdf format so as to have complete display of image.
From Silhouette plot, we can say that clustering is done in a proper format
###Dismilarity Matrix Calculation
```{r}
daisy_I <- cluster::daisy(
  x = scale_I[1:5,],
  metric = "gower"
)
print(daisy_I)

```

Summary of daisy
```{r}
summary(daisy_I)
```
ellipsoid function

```{r}
ellipsoidhull_I <- cluster::ellipsoidhull(
  x = scale_I
)
print(ellipsoidhull_I)

```
Cluster Volume

```{r}
cluster::volume(ellipsoidhull_I)
```
###Hierachical Clustering
We will use distance matrix to see which points falls in which clusters
```{r}
dist_I <- dist(scale_I)
pdf(file='C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/heatmap.pdf',height=100,width=100)
heatmap(
  x = as.matrix(dist_I),
  col = viridis::viridis(256),cexRow = 0.5,cexCol = 0.5
)
dev.off()
heatmap(
  x = as.matrix(dist_I),
  col = viridis::viridis(256),cexRow = 0.5,cexCol = 0.5
)
```

We can also use factoextra package for the same reason:
```{r}
require(ggrepel)
factoextra::fviz_dist(dist_I)
pdf(file='C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/factoe_heatmap.pdf',height=100,width=100)
factoextra::fviz_dist(dist_I)
dev.off()
```
We can use qgraph to understand the bond and relation between points as:

```{r}
install.packages('qgraph')
library(qgraph)
view(1/dist_I)
input=1/dist_I
input[!is.finite(input)] <- 0
qgraph::qgraph(
  input = input,
  layout="spring",
  minimum = 0.1
)

```
###Hierachical clustering algorithm

```{r}
hclust_I <- hclust(dist_I)
pdf(file='C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/cluster_dendogram.pdf',height = 30,width=80)
plot(hclust_I)
dev.off()
plot(hclust_I)
```
Using cutree to do PCA

```{r}
cutree_I <- cutree(
  tree = hclust_I,
  k = 4
)

prcomp_I <- data.frame(
  prcomp(
    x = scale_I,
    center = FALSE,
    scale. = FALSE
  )$x[,1:2],
  Age = temp_variable$age,
  Charges = temp_variable$charges,
  Cluster = as.character(cutree_I),
  stringsAsFactors = FALSE
)

require(ggplot2)
## Loading required package: ggplot2
ggplot(prcomp_I) + 
  aes(x = PC1,y = PC2,size = Charges,color = Cluster,fill = Cluster,label = "",group = Cluster) + 
  geom_point() + 
  ggrepel::geom_text_repel(color = "black",size = 3) + 
  ggtitle("Scatter plot of decathlon principal components","Color corresponds to k-means cluster") + 
  theme_bw() + 
  theme(legend.position = "top")

```

We can use Silhouette plot to see clustering 
```{r}
silhouette_I <- cluster::silhouette(
  x = cutree_I,
  dist = dist_I
)
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/silhouette_hclust.pdf')
plot(silhouette_I,)
dev.off()


```

USing distance Metrics to compute barious parameters as outier detction, equal size clusters and so on
```{r}
v_dist <- c(
  "canberra","manhattan","euclidean","maximum","minkowski"
)
list_dist <- lapply(
  X = v_dist,
  FUN = function(distance_method) dist(
    x = scale_I,
    method = distance_method
  )
)
names(list_dist) <- v_dist
v_hclust <- c(
  "ward.D","ward.D2","complete","mcquitty","average","single"
)

```

Selection of best clustering
```{r}
list_hclust <- list()
for(j in v_dist) for(k in v_hclust) list_hclust[[j]][[k]] <- hclust(
  d = list_dist[[j]],
  method = k
)
par(
  mfrow = c(length(v_dist),length(v_hclust)),
  mar = c(0,0,0,0),
  mai = c(0,0,0,0),
  oma = c(0,0,0,0)
)
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/criterioncluster.pdf',height=30,width=80)
for(j in v_dist) for(k in v_hclust) plot(
  x = list_hclust[[j]][[k]],
  labels = FALSE,
  axes = FALSE,
  main = paste("\n",j,"\n",k)
)
dev.off()

for(j in v_dist) for(k in v_hclust) plot(
  x = list_hclust[[j]][[k]],
  labels = FALSE,
  axes = FALSE,
  main = paste("\n",j,"\n",k)
)

```

Proper Split of dendograms
```{r}
for(j in v_dist) for(k in v_hclust) list_hclust[[j]][[k]]$height <- rank(list_hclust[[j]][[k]]$height)
par(
  mfrow = c(length(v_dist),length(v_hclust)),
  mar = c(0,0,0,0),
  mai = c(0,0,0,0),
  oma = c(0,0,0,0)
)
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/criterioncluster_ranked.pdf',height=30,width=80)
for(j in v_dist) for(k in v_hclust) plot(
  x = list_hclust[[j]][[k]],
  labels = FALSE,
  axes = FALSE,
  main = paste("\n",j,"\n",k)
)
dev.off()
for(j in v_dist) for(k in v_hclust) plot(
  x = list_hclust[[j]][[k]],
  labels = FALSE,
  axes = FALSE,
  main = paste("\n",j,"\n",k)
)
```

For equal number of clusters
```{r}
M_coef <- matrix(
  data = NA,
  nrow = length(v_dist),
  ncol = length(v_hclust)
)
rownames(M_coef) <- v_dist
colnames(M_coef) <- v_hclust
for(j in v_dist) for(k in v_hclust) try({
  M_coef[j,k] <- cluster::coef.hclust(
    object = list_hclust[[j]][[k]]
  )
})
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/equalsize_cluster.pdf',height=30,width=80)
plot(
  x = list_hclust[["canberra"]][["ward.D"]],
  main = "Canberrra Ward's D", hang = -1, cex = 0.6,
  sub = ""
)
dev.off()
plot(
  x = list_hclust[["canberra"]][["ward.D"]],
  main = "Canberrra Ward's D", hang = -1, cex = 0.6,
  sub = ""
)
```

For Outlier Detection
```{r}
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/OutlierDetection.pdf',height=30,width=80)
plot(
  x = list_hclust[["maximum"]][["single"]],
  main = "Maximum Single",
  sub = ""
)
dev.off()
plot(
  x = list_hclust[["maximum"]][["single"]],
  main = "Maximum Single",
  sub = ""
)
```
Cluster Functions for Hierachical Clustering involves agnes, diana and mona

```{r}
v <- prcomp(scale_I)$x[,1]
scale_I <- scale_I[order(v),]

```
For agnes:

```{r}
agnes_I <- cluster::agnes(scale_I)
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/agnes.pdf' )
plot(agnes_I)
dev.off()


```


```{r}
as.hclust(
  x = agnes_I
)

```


```{r}
print(agnes_I)
```
as Dendogram

```{r}
as.dendrogram(
  object = agnes_I
)

```
Cutree

```{r}
cutree(
  tree = agnes_I,
  k = 4
)

```

###Diana Function
```{r}
diana_I <- cluster::diana(scale_I)
pdf('C:/Users/shrut/Documents/DataMiningII-6704/Assignment2/diana.pdf')
plot(diana_I)
dev.off()


```


```{r}
as.hclust(
  x = diana_I
)

```
printing Diana

```{r}
print(diana_I)
```
As Dendogram

```{r}
as.dendrogram(
  object = diana_I
)

```
Cutree

```{r}
cutree(
  tree = diana_I,
  k = 4
)

```
###MOna Function

```{r}
view(scale_I)
binary_I <- scale_I
for(j in 1:ncol(binary_I)) binary_I[,j] <- as.numeric(
  binary_I[,j] > median(binary_I[,j])
)
mona_I <- cluster::mona(binary_I)
print(mona_I)

```

Plotting MOna
```{r}
plot(mona_I)
```

Complete
```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```


```{r}
```
